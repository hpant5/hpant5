# Hi, I'm Himanshu Pant

Data Applications Engineer | AI and Data Platforms | MS Software Engineering @ ASU

I build production-grade data and AI systems: from Spark and Snowflake ETL to agentic RAG applications with real users.

- Currently: MS in Software Engineering (AI specialization) at Arizona State University, building data applications and AI agents.
- Experience: 5+ years in data engineering across fintech, sports gaming, and healthcare using Spark, Snowflake, Airflow, Databricks, AWS, and Azure.
- Interests: Data platforms, machine learning in production, RAG systems, workflow orchestration.
- Certifications: AWS Certified, Microsoft Fabric Certified .
- Websites: Portfolio at https://himanshupant.dev, PauseButton at https://pausebutton.dev, LifeSync at https://lifesync.xyz.
- Location: Phoenix, AZ (open to relocation) 
- Contact: hpant.data@gmail.com
- LinkedIn: https://linkedin.com/in/itshimanshup
- GitHub: https://github.com/hpant5

---

## Featured Work

### AI Audit and Compliance Agent (AI Lawyer-style)

Repository: https://github.com/hpant5/AI_Audit_Compliance

Agentic AI law and compliance consultant with a RAG pipeline and ChromaDB, powered by large language models and a Streamlit user interface.  
- Uses vector search over compliance and policy documents to ground responses.  
- Implements an agentic workflow for question understanding, retrieval, and reasoning.  
- Built with Python, LLM tooling, ChromaDB, and Streamlit.

---

### Goods for Good – Donation Logistics Platform

Full-stack donation logistics application with real-time notifications and USPS API integration.
- Optimizes delivery routing using geolocation, improving routing time by approximately 30 percent.
- Handles donor to NGO matching, pickup scheduling, and tracking workflow end to end.
- Stack: describe frontend, backend, database, and hosting here.

---

### Industry Data and Machine Learning Systems

I have end-to-end data engineering and ML experience across sports gaming, insurance, and healthcare domains.

#### Sports Gaming and Fantasy Sports (Fantasy Akhanda)

At Fantasy Akhanda, I built the core data infrastructure to support sports prediction models across cricket, football, and basketball.  
- Designed and implemented multiple batch and streaming data pipelines ingesting semi-structured sports data into curated analytical layers.  
- Enabled model teams to build and deploy prediction models by standardizing schemas, automating feature computation, and improving data freshness and reliability.  
- Worked with technologies such as Spark, Databricks, Kafka, and cloud storage for scalable processing and delivery.

#### Insurance – Historical Feature Store and ML Pipelines

Domain: Insurance  
Technologies: AWS (S3, Athena, Glue, Lambda), Snowflake, PySpark, Kafka, Spark Streaming, Airflow

Objective: Creation of a historical feature store and end-to-end data pipelines for insurance machine learning use cases such as fraud detection, policy life expectancy, retention, and casualty reserving models.

- Data ingestion: Ingested data from legacy systems such as AS400 and Guidewire into an intermediate S3 layer in parquet format.  
- Data preparation and processing: Implemented raw to curated transformations in Spark, with data quality checks on incoming data and Airflow orchestration to manage end-to-end workflows.  
- Feature store: Built pipelines from the intermediate S3 layer into Snowflake, organizing partitioned historical features for the data science team.  
- Online feature store: Used Kafka for real-time streaming and Spark Streaming to transform JSON events into tabular features over different time windows, separating error records from valid records and optimizing Kafka and Spark Streaming performance.  
- Quality checks: Developed automated Python and PySpark test scripts for unit and integration testing, validating intermediate and final data frames for counts, aggregations, schema, column, and null checks.

#### Healthcare – SAS to Spark Migration and Databricks Workflows

Domain: Healthcare  
Technologies: Databricks, Spark, PySpark, Spark SQL, DB2, Snowflake, Teradata, Oracle, Airflow or Databricks Workflows

Objective: Migrate legacy SAS-based healthcare ETL pipelines to a modern Spark-based platform and significantly reduce processing time.

- Migration: Performed end-to-end SAS flow analysis and reimplemented logic using Spark SQL and PySpark notebooks, including dynamic date handling, macro-like parameterization, and reusable components over a medallion architecture.  
- Connectors: Created multiple source connectors to DB2, Snowflake, Teradata, and Oracle to support unified ingestion into the lakehouse.  
- Performance: Optimized pipelines to reduce runtime from approximately 9 hours to under 3 hours by improving partitioning, joins, and I/O patterns.  
- Data validation: Implemented automated validation for mappings, row counts, aggregation checks, and data quality checks (schema, columns, nulls) at intermediate and final table levels using Python-based scripts.

Across these roles, I have consistently owned design, implementation, and hardening of data platforms that support production-grade analytics and machine learning.


## Product and Web Applications

### PauseButton – Relax Hub for Developers

Live: https://pausebutton.dev

Web application where developers can relax with casual games and ambient sounds.  
- Includes games such as memory match, stack builder, and color switch, plus curated relaxing audio.  
- Built as a playground for frontend user experience, game mechanics, and state management.

### LifeSync – Full-stack Planning Assistant

Full-stack React and Node.js application that helps users plan and track life tasks.  
- Implements authentication, task and goal management, and calendar-style organization.  
- Designed to support future AI-assisted planning recommendations.

### JobGenie – AI-driven Job Application Agent (Planned)

Planned AI agent and resume parser that takes an initial resume and automates applying to similar jobs.  
- Uses an AI agent to parse resumes, analyze job descriptions, and generate tailored applications.  
- Designed as a subscription product with automated job search, application generation, and tracking.

---

## Tech Stack

Big Data and Analytics: Spark, PySpark, Spark SQL, Hive, Iceberg, Delta Lake  
Data Storage: ADLS Gen2, Amazon S3  
ETL and ELT: Databricks, Azure Data Factory, SSIS, SAS, AWS Glue, dbt  
Data Warehousing: Snowflake, Azure Synapse Analytics, PostgreSQL  
Data Analytics: Excel, MS SQL Server, Python, Pandas, NumPy  
NoSQL: MongoDB, DynamoDB  
Real-time Streaming: Kafka, Spark Streaming, Azure Stream Analytics  
Orchestration: Airflow, Databricks Workflows, AWS Step Functions, Azure Data Factory  
Visualization: Power BI, Tableau  
Infrastructure as Code: Terraform for AWS infrastructure creation  
Version Control: GitHub, Azure DevOps, Bitbucket  
CI/CD and DevOps: Docker, Azure DevOps pipelines, Linux, Jenkins  
Azure Data Services: Stream Analytics, Event Hubs, Function Apps, Key Vault  
AWS Data Services: EMR, Athena, Lambda, Elasticsearch, Step Functions, Batch, SES, SNS, IAM  
Project and Team Practices: Team leadership and technical mentoring, Jira, Agile (Scrum, sprint planning, review, and retrospective)


## GitHub at a Glance

![GitHub Stats](https://github-readme-stats.vercel.app/api?username=hpant5&show_icons=true&theme=transparent)  
![Top Languages](https://github-readme-stats.vercel.app/api/top-langs/?username=hpant5&layout=compact&theme=transparent)
